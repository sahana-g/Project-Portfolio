{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils import data as D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.datasets as datasets\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import numpy as np\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(4, 32, 5, padding = 2)\n",
    "        self.conv2 = nn.Conv1d(32, 64, 5, padding = 2)\n",
    "        self.conv3 = nn.Conv1d(64, 128, 5, padding = 2)\n",
    "        self.channel_red = nn.Conv1d(128, 1, 5, padding = 2)                                                                                                                                                                            \n",
    "        self.sig = nn.Sigmoid()\n",
    "        self.pool = nn.MaxPool1d(2)\n",
    "        #self.fc1 = nn.Linear(300, 300) ###same as self.channel_red layer \n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.channel_red(x)\n",
    "        #x = self.pool(x)\n",
    "        #x = self.conv2(x)\n",
    "        #x = F.relu(x)\n",
    "        #x = F.pool(x)\n",
    "        #x = self.softmax(x)\n",
    "        x = self.sig(x).squeeze()\n",
    "        ##output = F.log_softmax(x, dim=1)\n",
    "        return x\n",
    "\n",
    "net = NeuralNet()\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.0001)\n",
    "\n",
    "##use Adam optimizer, for loss can use ECE?? CrossEntropy or use BCU with ??? without sigmoid layer\n",
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data loader:\n",
    "from torch.utils import data as D\n",
    "def seq2onehot(seq):\n",
    "    window_size = 300\n",
    "    matrix = np.zeros(shape = (window_size, 4, 1))\n",
    "    for i, nt in enumerate(seq, 0):\n",
    "        if i == 300:\n",
    "            break\n",
    "        if nt == \"A\":\n",
    "            matrix[i][0][0] = 1\n",
    "        elif nt == \"G\":\n",
    "            matrix[i][1][0] = 1\n",
    "        elif nt == \"C\":\n",
    "            matrix[i][2][0] = 1\n",
    "        elif nt == \"U\":\n",
    "            matrix[i][3][0] = 1\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    return matrix\n",
    "\n",
    "class sbp_seq_data(D.Dataset):\n",
    "    def __init__(self, seq_path, training_mode = False, label_path = None):\n",
    "        self.seq_path = seq_path\n",
    "        self.seq_list = []\n",
    "        self.training_mode = training_mode\n",
    "        if self.training_mode:\n",
    "            self.label_dict = torch.load(label_path)\n",
    "\n",
    "        for line in open(self.seq_path, 'r'):\n",
    "            line = line.split('\\t')\n",
    "            self.seq_list.append((line[0], line[1]))\n",
    "        self.len = len(self.seq_list)\n",
    "        return\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sample = self.seq_list[index]\n",
    "        seq = sample[1]\n",
    "        seq_id = sample[0]\n",
    "        oh_seq = seq2onehot(seq)\n",
    "        oh_seq = torch.from_numpy(oh_seq.T).float()\n",
    "        if self.training_mode:\n",
    "            label = self.label_dict[seq]\n",
    "            label = torch.from_numpy((np.array(list(label), dtype=float))) #.reshape(-1, 1)\n",
    "\n",
    "            return oh_seq, label\n",
    "        else:\n",
    "            return seq_id, oh_seq\n",
    "\n",
    "    def __len__(self):\n",
    "        return(self.len)\n",
    "    \n",
    "\n",
    "#test_data = 'seq_vectors/Hela_pos_sample_seq_vectors_torchsave.pkl'\n",
    "all_data = sbp_seq_data(seq_path = 'sbp_polyA_RNA_seq_info/sbp_polyA_RNA_HEK293_mrna_seqs.tsv', training_mode = True, label_path = 'seq_vectors/HEK293_pos_sample_seq_vectors.pickle')\n",
    "\n",
    "train_data, test_data = torch.utils.data.random_split(all_data, [9785, 2446]) #generator = torch.Generator().manual_seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = D.DataLoader(train_data, batch_size=128, shuffle=True, num_workers=0)\n",
    "testloader = D.DataLoader(train_data, batch_size=128, shuffle=True, num_workers=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input dimension: \n",
    "inputs have dimenson 128 x 1 x 4 x 300 --> squeeze so inputs have dimension 128 x 4 x 300\n",
    "outputs have dimension 128 x 300\n",
    "labels have dimension 128 x 300 \n",
    "Same dimensions for BCE loss \n",
    "\n",
    "ISSUE: loss not changing, also number correct is = 0??? Is it an issue with how I'm counting the ones that are correct?\n",
    "Also, I rounded to see what was correct... is this causing issues because I have decimals at the end????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##training data\n",
    "for epoch in range(0, 8):  # loop over the dataset multiple times\n",
    "    correct =0\n",
    "    total =0\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.squeeze(1)\n",
    "        labels = labels.float()\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs.float())\n",
    "        loss = criterion(outputs.float(), labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 4 == 3:    # print every 4 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 4:.3f}')\n",
    "            running_loss = 0.0\n",
    "            \n",
    "        total += len(outputs)\n",
    "        outputs = torch.round(outputs)\n",
    "        ##check outputs vs. label row by row in batch\n",
    "        for j in range(0, len(outputs)):\n",
    "            if torch.equal(outputs[j], labels[j]):\n",
    "                correct +=1\n",
    "\n",
    "    print(100*correct/total)\n",
    "print('Finished Training')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##testing\n",
    "correct = 0\n",
    "total = 0\n",
    "# since we're not training, we don't need to calculate the gradients for our outputs\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(testloader):\n",
    "        test_seqs, labels = data\n",
    "        labels = labels.float()\n",
    "        labels = labels.squeeze(1)\n",
    "        test_seqs = test_seqs.squeeze()\n",
    "        # calculate outputs by running images through the network\n",
    "        outputs = net(test_seqs)\n",
    "        \n",
    "        # the class with the highest val is what we choose as prediction\n",
    "        predicted = torch.max(outputs.data)\n",
    "\n",
    "        total += len(outputs)\n",
    "        \n",
    "        outputs = torch.round(outputs)\n",
    "        ##check each row in batch add to correct if correct\n",
    "        for j in range(0, len(outputs)):\n",
    "            if torch.equal(outputs[j], labels[j]):\n",
    "                correct +=1\n",
    "print(f'Accuracy of the network on the test vectors: {100 * correct/ total} %')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####positive A percentage calculator: before running model\n",
    "'''\n",
    "def DRACH_motif():\n",
    "    D = ['G', 'A', 'U']\n",
    "    R = ['G', 'A']\n",
    "    H = ['A', 'U', 'C']\n",
    "    motifs = []\n",
    "    for i in D:\n",
    "        for j in R:\n",
    "            for k in H:\n",
    "                motifs.append(i + j + 'A' + 'C' + k)\n",
    "    return motifs\n",
    "\n",
    "def count_motifs(seq, motifs):\n",
    "    nums = 0\n",
    "    for m in motifs:\n",
    "        nums += seq.count(m)\n",
    "    return nums\n",
    "\n",
    "percentages = [\n",
    "drach_motifs = DRACH_motif()\n",
    "with open('sbp_polyA_RNA_seq_info/sbp_polyA_RNA_Hela_mrna_seqs.tsv', 'r') as f:\n",
    "    for line in f:\n",
    "        line = line.split('\\t')\n",
    "        cur_seq = line[1]\n",
    "        num_As = count_motifs(cur_seq, drach_motifs)\n",
    "        percentages.append((num_As / 300) * 100)\n",
    "    print(percentages)\n",
    "        \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''MASKING: FOR LATER\n",
    "test_in = torch.randint(2, (1, 4, 300), dtype=torch.float)\n",
    "print(test_in)\n",
    "mask = test_in.eq(0) ##MASK OUT EVERYTHING THAT IS NOT As, NOT JUST NOT M6A. (like all As should be left)\n",
    "#print(mask)\n",
    "test_in = torch.masked_fill(test_in, mask, value = 0)\n",
    "print(test_in.size())\n",
    "model = NeuralNet()\n",
    "out = model(test_in)\n",
    "\n",
    "##print(test_in.size())\n",
    "print(\"out: \", out.size())\n",
    "#print(test_in == out)\n",
    "\n",
    "x = torch.rand(3, 4)\n",
    "print(x)\n",
    "mask = x.eq(1)\n",
    "print(mask)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
